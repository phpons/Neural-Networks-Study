{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_Definition",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phpons/Neural-Networks-Study/blob/master/NN_Definition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3PYHt_ny-CB"
      },
      "source": [
        "# Classes Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFEy2L0DKmvw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.output = 0\n",
        "\n",
        "    def activation_function(self, z): # Switch between RELU and Sigmoid as wanted\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def relu(self, z):\n",
        "        return max(0, z)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        if abs(z) > 10:\n",
        "          # Avoids overflow\n",
        "          out = 1 if z > 0 else 0\n",
        "        else:\n",
        "          out = 1.0 / (1.0 + np.exp(-z))\n",
        "        return out\n",
        "\n",
        "    def de_dz(self, de_dy):\n",
        "        return (self.output * (1- self.output)) * de_dy\n",
        "\n",
        "    def de_dz_relu(self, de_dy):\n",
        "        if self.output > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def z(self, inputs):\n",
        "        sum_input = 0\n",
        "        for i, weight in enumerate(self.weights):\n",
        "            sum_input += weight * inputs[i]\n",
        "        return sum_input + self.bias\n",
        "\n",
        "    def y(self, inputs):\n",
        "        self.output = self.activation_function(self.z(inputs))\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class DenseLayer:\n",
        "\n",
        "    def __init__(self, neurons):\n",
        "        self.neurons = neurons\n",
        "        self.outputs = []\n",
        "        self.outputs = np.array(self.outputs)\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        outputs = []\n",
        "\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            outputs.append(neuron.y(inputs))\n",
        "\n",
        "        outputs = np.array(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes) - 1\n",
        "        self.layers = []\n",
        "\n",
        "        # Generates weights and biases\n",
        "        weights_all = [np.random.randn(y, x)\n",
        "                   for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        weights_all[:] = [x / 12 for x in weights_all]\n",
        "\n",
        "        biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "        for i in range(self.num_layers): # Creates layers\n",
        "            neurons = []\n",
        "\n",
        "            for j in range(sizes[i+1]):\n",
        "                weights = np.random.uniform(-1, 1, sizes[i])\n",
        "                neuron = Neuron(weights_all[i][j], biases[i][j][0])\n",
        "                neurons.append(neuron)\n",
        "\n",
        "            self.layers.append(DenseLayer(neurons))\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        pass_forward = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            pass_forward = self.layers[i].feed_forward(pass_forward)\n",
        "        return pass_forward\n",
        "\n",
        "    def cost_function(self, output, expected_output):\n",
        "        output_array = np.array(output)\n",
        "        expected_output_array = np.array(expected_output)\n",
        "\n",
        "        sq_error = (expected_output_array - output_array) ** 2\n",
        "        mean_sq_error = np.sum(sq_error) / (sq_error.size*2)\n",
        "\n",
        "        return mean_sq_error\n",
        "\n",
        "    def de_dy(self, expected_output, output): #DE/DYj\n",
        "        return -1 * (expected_output - output)\n",
        "\n",
        "    def backprop(self, expected_output, output, inputs, learning_rate):\n",
        "        de_dy_array = self.de_dy(expected_output, output)\n",
        "\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            if i != 0:\n",
        "                for j, neuron in enumerate(self.layers[i].neurons):\n",
        "                    if i == (len(self.layers) - 1): # Last layer\n",
        "                        de_dz = neuron.de_dz(de_dy_array[j])\n",
        "                    else:\n",
        "                        de_dz = neuron.de_dz_relu(de_dy_array[j])\n",
        "                    de_dyi = np.zeros(len(neuron.weights)) # Derivative from previous layer\n",
        "\n",
        "                    for idx, w in enumerate(neuron.weights):\n",
        "                        de_dw = de_dz * self.layers[i - 1].neurons[idx].output # de_dz * Yi\n",
        "                        de_dyi[idx] += de_dz * w\n",
        "                        self.layers[i].neurons[j].weights[idx] -= learning_rate * de_dw # Updates weights\n",
        "\n",
        "                    self.layers[i].neurons[j].bias -= learning_rate * de_dz # Updates biases\n",
        "                de_dy_array = np.copy(de_dyi)\n",
        "\n",
        "            else: #já que input não está em self.layers, preciso deste else pra pegar os inputs\n",
        "                for j, neuron in enumerate(self.layers[i].neurons):\n",
        "                    de_dz = neuron.de_dz(de_dy_array[j])\n",
        "                    de_dyi = np.zeros(len(neuron.weights))  # Derivative from previous layer\n",
        "\n",
        "                    for idx, w in enumerate(neuron.weights):\n",
        "                        de_dw = de_dz * inputs[idx]  # de_dz * Yi\n",
        "                        self.layers[i].neurons[j].weights[idx] -= learning_rate * de_dw  # Updates weights\n",
        "\n",
        "                    self.layers[i].neurons[j].bias -= learning_rate * de_dz  # Updates biases\n",
        "                de_dy_array = np.copy(de_dyi)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
